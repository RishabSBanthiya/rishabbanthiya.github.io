# Alternative Dockerfile with pre-downloaded model
# Stage 1: Build frontend
FROM node:18-alpine AS frontend-builder
WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY src/ ./src/
COPY public/ ./public/
COPY index.html.template ./index.html
COPY vite.config.ts ./
COPY tsconfig.json ./
COPY tsconfig.node.json ./
RUN npm run build

# Stage 2: Ollama + Nginx + Frontend (with pre-downloaded model)
FROM ubuntu:22.04 AS final

# Cache busting - force rebuild when base image changes
ARG CACHE_BUST=1

# Set non-interactive mode
ENV DEBIAN_FRONTEND=noninteractive

# Install dependencies
RUN apt-get update && \
    apt-get install -y \
        curl \
        nginx \
        wget \
        ca-certificates \
        systemd \
        systemctl \
        && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install Ollama using a more reliable method
RUN ARCH=$(uname -m) && \
    case $ARCH in \
        x86_64) ARCH="amd64" ;; \
        aarch64) ARCH="arm64" ;; \
        armv7l) ARCH="arm" ;; \
        *) echo "Unsupported architecture: $ARCH"; exit 1 ;; \
    esac && \
    OLLAMA_VERSION="0.1.30" && \
    DOWNLOAD_URL="https://github.com/ollama/ollama/releases/download/v${OLLAMA_VERSION}/ollama-linux-${ARCH}" && \
    echo "Downloading Ollama from: $DOWNLOAD_URL" && \
    curl -L --connect-timeout 60 --max-time 600 --retry 3 --retry-delay 5 "$DOWNLOAD_URL" -o /usr/local/bin/ollama && \
    chmod +x /usr/local/bin/ollama && \
    ollama --version

# Start Ollama service to pre-download the model
RUN ollama serve &
RUN sleep 10 && \
    ollama pull llama3.2:1b && \
    pkill ollama

# Copy frontend
COPY --from=frontend-builder /app/dist /usr/share/nginx/html

# Copy configs and scripts
COPY nginx.conf /etc/nginx/nginx.conf
COPY ollama-models/ /root/.ollama/models/
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Ports
EXPOSE 80 11434

# Health check
HEALTHCHECK CMD curl -f http://localhost:11434/api/version || exit 1

ENTRYPOINT ["/bin/bash", "/start.sh"]
