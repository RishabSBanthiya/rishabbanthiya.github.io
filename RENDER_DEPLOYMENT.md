# Deploying to Render with AI Fallback

This guide explains how to deploy your portfolio to Render with the hybrid AI system.

## How the AI System Works

Your portfolio now has a **hybrid AI system**:

### Development (Local)
- ‚úÖ **Ollama LLM** (when available) - Advanced AI with natural language understanding
- üöÄ Runs locally, free, powerful responses

### Production (Render)
- ‚úÖ **Smart Fallback** - Pattern-matching AI that doesn't require Ollama
- üåê Works on static hosting, no server needed
- üì¶ Built into the frontend bundle

## The Hybrid System

The AI automatically detects if Ollama is available:

```typescript
1. Try to connect to Ollama (localhost:11434)
   ‚îú‚îÄ If available ‚Üí Use Ollama LLM (best experience)
   ‚îî‚îÄ If unavailable ‚Üí Use fallback pattern matching (still smart!)
```

### What Users See

**With Ollama (Development):**
```
shrub's bot (powered by Ollama):
[Natural language response streaming in real-time]
```

**Without Ollama (Production):**
```
shrub's bot (smart mode):
[Pattern-matched response with typing animation]
```

Both modes answer questions accurately using your knowledge base!

## Deployment to Render

### Option 1: Static Site (Recommended)

This is the easiest option and uses the fallback AI system.

#### Step 1: Build for Production

```bash
npm run build
```

This creates a `dist/` folder with your built site.

#### Step 2: Deploy to Render

1. Go to [Render Dashboard](https://dashboard.render.com/)
2. Click **"New +"** ‚Üí **"Static Site"**
3. Connect your GitHub repository
4. Configure:
   - **Build Command:** `npm run build`
   - **Publish Directory:** `dist`
   - **Auto-Deploy:** Yes (optional)

#### Step 3: Deploy!

Click **"Create Static Site"** and Render will:
- Install dependencies
- Build your project
- Deploy to a URL like `https://your-site.onrender.com`

### Option 2: Docker with Ollama (Advanced)

If you want Ollama running in production (requires paid Render plan with Docker):

#### Step 1: Create Dockerfile

Create `/Users/rishabbanthiya/Desktop/rishabbanthiya.github.io/Dockerfile`:

```dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build

FROM ollama/ollama:latest

# Install nginx to serve static files
RUN apt-get update && apt-get install -y nginx curl

# Copy built frontend
COPY --from=builder /app/dist /usr/share/nginx/html

# Copy nginx config
COPY nginx.conf /etc/nginx/nginx.conf

# Pull Ollama model
RUN ollama pull llama3.2:3b

# Copy and build custom model
COPY ollama-models/rishab-bot.Modelfile /tmp/
RUN ollama create rishab-bot -f /tmp/rishab-bot.Modelfile

# Expose ports
EXPOSE 80 11434

# Start script
COPY start.sh /start.sh
RUN chmod +x /start.sh

CMD ["/start.sh"]
```

‚ö†Ô∏è **Note:** This requires significant resources and a paid Render plan. The fallback system works great for most use cases!

## Testing the Fallback System

### Test Locally (Without Ollama)

1. Stop Ollama:
   ```bash
   # Kill any running Ollama processes
   pkill ollama
   ```

2. Start dev server:
   ```bash
   npm run dev
   ```

3. Open http://localhost:5173

4. Try the AI:
   ```
   ai What are Rishab's skills?
   ```

You should see: `shrub's bot (smart mode):` indicating fallback is working!

### Test on Render

After deploying, visit your Render URL and test the AI commands:

```
ai Tell me about Rishab's experience
ai What technologies does he use?
ai How can I contact him?
```

All should work perfectly with the fallback system!

## Updating Your AI's Knowledge

The knowledge base is shared between Ollama and fallback modes.

### Step 1: Edit Knowledge Base

Edit `src/components/Terminal.tsx` (lines 291-334):

```typescript
const aiKnowledgeBase = {
  personal: {
    // Update your info here
  },
  skills: {
    // Update your skills
  },
  // ... etc
}
```

### Step 2: For Development (Ollama)

Rebuild the Ollama model:
```bash
npm run build:ollama
```

### Step 3: For Production (Render)

Just commit and push:
```bash
git add src/components/Terminal.tsx
git commit -m "Update AI knowledge base"
git push
```

Render will automatically rebuild and deploy!

## Environment Variables

No environment variables needed! The system auto-detects Ollama availability.

Optional (if using Render environment variables):
```bash
# Optional: Force fallback mode even if Ollama is available
FORCE_AI_FALLBACK=true
```

## Performance Comparison

| Feature | Ollama (Dev) | Fallback (Prod) |
|---------|-------------|-----------------|
| Speed | 1-3s first response, <1s after | Instant (<100ms) |
| Understanding | Natural language | Keyword matching |
| Accuracy | High | High (for known queries) |
| Server Required | Yes (local) | No |
| Cost | Free (local) | Free (static) |
| Works Offline | Yes | Yes |

## Troubleshooting

### "AI not responding" on Render

**Cause:** Fallback system not working
**Solution:** Check browser console for errors

### "Ollama mode" showing on Render

**Cause:** Shouldn't happen, but indicates Ollama detection issue
**Solution:** Clear browser cache and reload

### Slow responses on Render

**Cause:** Normal - pattern matching is instant, typing animation adds delay
**Solution:** This is intentional for better UX (simulates streaming)

### AI giving wrong answers

**Cause:** Knowledge base needs updating
**Solution:** Update `aiKnowledgeBase` in Terminal.tsx

## Comparison: Development vs Production

### Development (with Ollama)
```bash
# Start Ollama
ollama serve

# Start dev server
npm run dev
```

**AI Features:**
- ‚úÖ Natural language understanding
- ‚úÖ Context-aware responses
- ‚úÖ Handles complex questions
- ‚úÖ Real-time streaming

### Production (Render - no Ollama)
```bash
# Just deploy!
git push
```

**AI Features:**
- ‚úÖ Fast keyword matching
- ‚úÖ Accurate for common questions
- ‚úÖ Typing animation
- ‚úÖ No server needed

## Best Practices

### 1. Keep Knowledge Base Updated

Update `aiKnowledgeBase` in Terminal.tsx regularly. Changes automatically work in both modes!

### 2. Test Both Modes

Before deploying:
```bash
# Test with Ollama
ollama serve
npm run dev

# Test fallback (stop Ollama)
pkill ollama
npm run dev
```

### 3. Monitor Usage

Check Render analytics to see how visitors use the AI feature.

### 4. Document Your Setup

Let visitors know the AI is powered by local tech (in footer or about section).

## Advanced: Custom Render Configuration

### render.yaml

Create `render.yaml` for automated deployments:

```yaml
services:
  - type: web
    name: rishab-portfolio
    env: static
    buildCommand: npm install && npm run build
    staticPublishPath: ./dist
    routes:
      - type: rewrite
        source: /*
        destination: /index.html
```

Then push and Render auto-deploys!

## FAQ

**Q: Will the AI work on Render?**
A: Yes! It automatically uses the fallback system.

**Q: Do I need to configure anything special?**
A: No! It just works. The fallback is built into the frontend.

**Q: Can I use Ollama on Render?**
A: Technically yes (with Docker + paid plan), but the fallback works great for most needs.

**Q: How do I update AI responses?**
A: Edit `aiKnowledgeBase` in Terminal.tsx and redeploy.

**Q: Is the fallback as good as Ollama?**
A: Different strengths:
- Ollama: Better natural language understanding
- Fallback: Faster, works everywhere, still accurate

**Q: Can users tell which mode is being used?**
A: Yes! The prompt shows:
- "powered by Ollama" = Using LLM
- "smart mode" = Using fallback

## Next Steps

1. ‚úÖ Deploy to Render
2. Test the AI on your live site
3. Share your portfolio!
4. Update knowledge base as you grow

Your portfolio now works perfectly in both development and production! üéâ

